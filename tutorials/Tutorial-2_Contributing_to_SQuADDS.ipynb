{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/shanto/LFL/SQuADDS/SQuADDS\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: SQuADDS\n",
      "  Attempting uninstall: SQuADDS\n",
      "    Found existing installation: SQuADDS 0.1\n",
      "    Uninstalling SQuADDS-0.1:\n",
      "      Successfully uninstalled SQuADDS-0.1\n",
      "  Running setup.py develop for SQuADDS\n",
      "Successfully installed SQuADDS-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ../."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Contributing to SQuADDS\n",
    "\n",
    "\n",
    "**Table of Contents:**\n",
    "\n",
    "0. [Contribution Information Setup](#setup)\n",
    "1. [Understanding the terminology and database structure](#structure)\n",
    "2. [Contributing to an existing database node](#contribution)\n",
    "3. [Creating new database node](#creation)\n",
    "4. [Building on top of others works](#missing)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution Information Setup <a name=\"setup\"></a>\n",
    "\n",
    "In order to contribute to SQuADDS, you will need to provide some information about yourself. This information will be used to track your contributions and to give you credit for your work. You can provide this information by updating the following variables in the `.env` file in the root directory of the repository:\n",
    "\n",
    "```\n",
    "GROUP_NAME = \"\"\n",
    "PI_NAME = \"\"\n",
    "INSTITUTION = \"\"\n",
    "USER_NAME = \"\"\n",
    "```\n",
    "\n",
    "Or you can provide this information by executing the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from squadds.database import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_contributor_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the terminology and database structure <a name=\"structure\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HuggingFace\n",
    "- Datasets\n",
    "- Configurations\n",
    "- Structure of SQuADDS_DB\n",
    "- Adding to SQuADDS_DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Processing:\n",
    "\n",
    "We want the data to be in a `json` format with **AT LEAST** to have the following fields. You can add as many more supplementary fields as you want.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"design\":{\n",
    "        \"design_options\": design_options,\n",
    "        \"design_tool\": design_tool_name, \n",
    "    },\n",
    "    \"sim_options\":{\n",
    "        \"setup\": sim_setup_options,\n",
    "        \"simulator\": simulator_name,\n",
    "    },\n",
    "    \"sim_results\":{\n",
    "        \"result1\": sim_result1,\n",
    "        \"unit1\": unit1,\n",
    "        \"result2\": sim_result2,\n",
    "        \"unit2\": unit2,\n",
    "    },\n",
    "    \"contributor\":{\n",
    "        \"group\": group_name,\n",
    "        \"PI\": pi_name,\n",
    "        \"institution\": institution,\n",
    "        \"uploader\": user_name,\n",
    "        \"date_created\": \"YYYY-MM-DD-HHMMSS\",\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "If all the `sim_results` has the same units you can just use a `\"units\":units` field instead of repeating the unit for each result.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding to an Existing Configuration <a name=\"contribution\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Clone/Fork the Repository**: If you have not already forked or cloned the repository, do so.\n",
    "\n",
    "2. **Create or Checkout a Branch**: If adding new data, it might be best to do it on a new branch:\n",
    "\n",
    "   ```sh\n",
    "   git checkout -b add_to_configuration\n",
    "   ```\n",
    "\n",
    "3. **Modify the Configuration**: Add or modify the data files as necessary for the configuration. Make sure to follow any guidelines provided by the dataset maintainers for the specific structure and format required.\n",
    "\n",
    "4. **Commit and Push Your Changes**: Commit the new data and push it to your fork:\n",
    "\n",
    "   ```sh\n",
    "   git add .\n",
    "   git commit -m \"Add new data to configuration Y\"\n",
    "   git push origin add_to_configuration\n",
    "   ```\n",
    "\n",
    "5. **Pull Request**: Create a pull request against the original repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributing a New Configuration <a name=\"creation\"></a>\n",
    "\n",
    "1. **Fork the Dataset Repository**: On the Hugging Face Hub, fork the dataset repository you want to contribute to.\n",
    "\n",
    "2. **Clone Your Fork Locally**: Clone the forked repository to your local machine using the following command:\n",
    "\n",
    "   ```sh\n",
    "   git clone https://huggingface.co/datasets/YOUR_USERNAME/DATASET_NAME\n",
    "   ```\n",
    "\n",
    "3. **Create a New Branch**: It's a good practice to create a new branch for your configuration contribution:\n",
    "\n",
    "   ```sh\n",
    "   git checkout -b new_configuration\n",
    "   ```\n",
    "\n",
    "4. **Add Your Configuration**: Depending on the dataset's structure, this might involve adding new files or modifying existing ones. If the dataset uses the `datasets` library's builder configurations, you will need to modify the Python script that defines the configurations.\n",
    "\n",
    "5. **Commit Your Changes**: Commit the changes with a clear commit message:\n",
    "\n",
    "   ```sh\n",
    "   git add .\n",
    "   git commit -m \"Add new configuration for circuit element X\"\n",
    "   ```\n",
    "\n",
    "6. **Push to Your Fork**: Push your new branch to your fork on the Hugging Face Hub:\n",
    "\n",
    "   ```sh\n",
    "   git push origin new_configuration\n",
    "   ```\n",
    "\n",
    "7. **Create a Pull Request**: Go to the Hugging Face Hub, navigate to your fork, and create a pull request for your new branch. The pull request will be reviewed by the dataset maintainers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Qiskit Metal Rendering Code to get `design`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Simulation Code to get `sim_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of SQuADDS API to upload dataset - adds `contributor` information automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the `load_dataset` config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload to `SQuADDS_DB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_json_files(source_directory, output_file):\n",
    "    all_data = []\n",
    "\n",
    "    # List all JSON files in the directory\n",
    "    file_paths = glob.glob(os.path.join(source_directory, \"*.json\"))\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            all_data.append(data)\n",
    "\n",
    "    # Write combined data to a single file\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(all_data, outfile, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"../data/qubit/TransmonCross/cap_matrix\"  # Replace with your source directory\n",
    "output_file = \"../data/combined_qubit_data.json\"  # Replace with your desired output file path\n",
    "combine_json_files(source_directory, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"../data/coupler/NCap/cap_matrix\"  # Replace with your source directory\n",
    "output_file = \"../data/combined_coupler_data.json\"  # Replace with your desired output file path\n",
    "combine_json_files(source_directory, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"../data/cavity_claw/RouteMeander/eigenmode/\"  # Replace with your source directory\n",
    "output_file = \"../data/combined_cavity-claw_data.json\"  # Replace with your desired output file path\n",
    "combine_json_files(source_directory, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the json files in data to \"LFL_USC_{hash}.json\" where the hash is based on the file contents\n",
    "\n",
    "for file in glob.glob(\"../data/*.json\"):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        hash = hashlib.md5(json.dumps(data, sort_keys=True).encode('utf-8')).hexdigest()\n",
    "        new_file = file.replace(\".json\", f\"LFL_USC_{hash}.json\")\n",
    "        os.rename(file, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /Users/shanto/.cache/huggingface/modules/datasets_modules/datasets/SQuADDS_DB/6c042e99be0aa47463aa5d9ae2ceeb87b02331a7e2aa159865c27f7b19e9316e\n",
      "Testing builder 'qubit-TransmonCross-cap_matrix' (1/3)\n",
      "Generating dataset s_qu_adds_db (/Users/shanto/.cache/huggingface/datasets/s_qu_adds_db/qubit-TransmonCross-cap_matrix/1.0.0/6c042e99be0aa47463aa5d9ae2ceeb87b02331a7e2aa159865c27f7b19e9316e)\n",
      "Downloading and preparing dataset s_qu_adds_db/qubit-TransmonCross-cap_matrix to /Users/shanto/.cache/huggingface/datasets/s_qu_adds_db/qubit-TransmonCross-cap_matrix/1.0.0/6c042e99be0aa47463aa5d9ae2ceeb87b02331a7e2aa159865c27f7b19e9316e...\n",
      "Downloading data files: 100%|██████████████████| 3/3 [00:00<00:00, 12458.33it/s]\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|█████████████████████| 3/3 [00:00<00:00, 853.43it/s]\n",
      "Generating train split\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shanto/miniconda3/envs/qiskit_metal/bin/datasets-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/shanto/miniconda3/envs/qiskit_metal/lib/python3.10/site-packages/datasets/commands/datasets_cli.py\", line 39, in main\n",
      "    service.run()\n",
      "  File \"/Users/shanto/miniconda3/envs/qiskit_metal/lib/python3.10/site-packages/datasets/commands/test.py\", line 146, in run\n",
      "    builder.download_and_prepare(\n",
      "  File \"/Users/shanto/miniconda3/envs/qiskit_metal/lib/python3.10/site-packages/datasets/builder.py\", line 948, in download_and_prepare\n",
      "    self._download_and_prepare(\n",
      "  File \"/Users/shanto/miniconda3/envs/qiskit_metal/lib/python3.10/site-packages/datasets/builder.py\", line 1043, in _download_and_prepare\n",
      "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
      "  File \"/Users/shanto/miniconda3/envs/qiskit_metal/lib/python3.10/site-packages/datasets/builder.py\", line 1445, in _prepare_split\n",
      "    raise NotImplementedError()\n",
      "NotImplementedError\n"
     ]
    }
   ],
   "source": [
    "!datasets-cli test ../data/SQuADDS_DB.py --save_info --all_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "def create_train_val_test_splits(source_files, output_directory, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Splits the data from source JSON files into train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    # Load data from all source files\n",
    "    for file_path in source_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            all_data.extend(data)\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(all_data)\n",
    "\n",
    "    # Split the data\n",
    "    total_data = len(all_data)\n",
    "    train_end = int(total_data * train_ratio)\n",
    "    val_end = train_end + int(total_data * val_ratio)\n",
    "\n",
    "    train_data = all_data[:train_end]\n",
    "    val_data = all_data[train_end:val_end]\n",
    "    test_data = all_data[val_end:]\n",
    "\n",
    "    # Save the splits\n",
    "    with open(os.path.join(output_directory, 'train.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_data, f, indent=4)\n",
    "\n",
    "    with open(os.path.join(output_directory, 'validation.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(val_data, f, indent=4)\n",
    "\n",
    "    with open(os.path.join(output_directory, 'test.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_data, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files = [\n",
    "    \"/Users/shanto/LFL/SQuADDS/SQuADDS/data/cavity_claw/LFL_USC_cavity_claw_3e95ba4a2e4da2141f9edaa9f9fa1653.json\",\n",
    "    \"/Users/shanto/LFL/SQuADDS/SQuADDS/data/coupler/LFL_USC_coupler_e7855e5c7467f76edb09779d8f3a1a0c.json\",\n",
    "    \"/Users/shanto/LFL/SQuADDS/SQuADDS/data/qubit/LFL_USC_qubit_e68f323df894ba4b2891bd64742a2c35.json\",\n",
    "]\n",
    "output_directory = '../data/'\n",
    "\n",
    "create_train_val_test_splits(source_files, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
