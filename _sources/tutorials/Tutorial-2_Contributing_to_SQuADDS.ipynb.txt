{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/shanto/LFL/SQuADDS/SQuADDS\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: SQuADDS\n",
      "  Attempting uninstall: SQuADDS\n",
      "    Found existing installation: SQuADDS 0.1\n",
      "    Uninstalling SQuADDS-0.1:\n",
      "      Successfully uninstalled SQuADDS-0.1\n",
      "  Running setup.py develop for SQuADDS\n",
      "Successfully installed SQuADDS-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ../."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Contributing to SQuADDS\n",
    "\n",
    "\n",
    "**Table of Contents:**\n",
    "\n",
    "0. [Contribution Information Setup](#setup)\n",
    "1. [Understanding the terminology and database structure](#structure)\n",
    "2. [Contributing to an existing database node](#contribution)\n",
    "3. [Creating new database node](#creation)\n",
    "4. [Building on top of others works](#missing)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution Information Setup <a name=\"setup\"></a>\n",
    "\n",
    "In order to contribute to SQuADDS, you will need to provide some information about yourself. This information will be used to track your contributions and to give you credit for your work. You can provide this information by updating the following variables in the `.env` file in the root directory of the repository:\n",
    "\n",
    "```\n",
    "GROUP_NAME = \"\"\n",
    "PI_NAME = \"\"\n",
    "INSTITUTION = \"\"\n",
    "USER_NAME = \"\"\n",
    "```\n",
    "\n",
    "Or you can provide this information by executing the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from squadds.database import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_contributor_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the terminology and database structure <a name=\"structure\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HuggingFace\n",
    "- Datasets\n",
    "- Configurations\n",
    "- Structure of SQuADDS_DB\n",
    "- Adding to SQuADDS_DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributing to an existing database node <a name=\"contribution\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Native Solution:\n",
    "\n",
    "**To append to an existing configuration:**\n",
    "\n",
    "1. Modify the existing `YourDatasetConfig` if needed.\n",
    "2. Update the `_split_generators` and `_generate_examples` methods to include your additions.\n",
    "3. Test your changes locally to ensure the dataset loads correctly.\n",
    "4. Update the documentation (`README.md`) to include your new configuration.\n",
    "5. Create a pull request with your changes.\n",
    "\n",
    "### SQuADDS API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new database node <a name=\"creation\"></a>\n",
    "\n",
    "### HuggingFace Native Solution:\n",
    "\n",
    "One can use workflows developed by HuggingFace to create a new **configuration** to the SQuADDS_DB Dataset. The following is a list of steps to follow: \n",
    "\n",
    "**To add a new configuration:**\n",
    "\n",
    "1. Add a new `YourDatasetConfig` to `BUILDER_CONFIGS` with a unique name, version, and description.\n",
    "2. Update `_URLs` with the download URL for the new configuration.\n",
    "3. Modify `_split_generators` and `_generate_examples` methods to handle the new configuration.\n",
    "4. Test your changes locally to ensure the dataset loads correctly.\n",
    "5. Update the documentation (`README.md`) to include your new configuration.\n",
    "6. Create a pull request with your changes.\n",
    "\n",
    "\n",
    "### SQuADDS API:\n",
    "\n",
    "You can also use the SQuADDS API to create a new configuration. The following cells will walk you through the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Standardizing your data\n",
    "\n",
    "We want the data to be in a `json` format with **AT LEAST** to have the following fields. You can add as many more supplementary fields as you want.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"design\":{\n",
    "        \"design_options\": design_options,\n",
    "        \"design_tool\": design_tool_name, \n",
    "    },\n",
    "    \"sim_options\":{\n",
    "        \"setup\": sim_setup_options,\n",
    "        \"simulator\": simulator_name,\n",
    "    },\n",
    "    \"sim_results\":{\n",
    "        \"result1\": sim_result1,\n",
    "        \"unit1\": unit1,\n",
    "        \"result2\": sim_result2,\n",
    "        \"unit2\": unit2,\n",
    "    },\n",
    "    \"contributor\":{\n",
    "        \"group\": group_name,\n",
    "        \"PI\": pi_name,\n",
    "        \"institution\": institution,\n",
    "        \"uploader\": user_name,\n",
    "        \"date_created\": \"YYYY-MM-DD-HHMMSS\",\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "If all the `sim_results` has the same units you can just use a `\"units\":units` field instead of repeating the unit for each result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Qiskit Metal Rendering Code to get `design`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Simulation Code to get `sim_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of SQuADDS API to upload dataset - adds `contributor` information automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the `load_dataset` config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload to `SQuADDS_DB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_json_files(source_directory, output_file):\n",
    "    all_data = []\n",
    "\n",
    "    # List all JSON files in the directory\n",
    "    file_paths = glob.glob(os.path.join(source_directory, \"*.json\"))\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            all_data.append(data)\n",
    "\n",
    "    # Write combined data to a single file\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(all_data, outfile, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"../data/qubit/TransmonCross/cap_matrix\"  # Replace with your source directory\n",
    "output_file = \"../data/combined_qubit_data.json\"  # Replace with your desired output file path\n",
    "combine_json_files(source_directory, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"../data/coupler/NCap/cap_matrix\"  # Replace with your source directory\n",
    "output_file = \"../data/combined_coupler_data.json\"  # Replace with your desired output file path\n",
    "combine_json_files(source_directory, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"../data/cavity_claw/RouteMeander/eigenmode/\"  # Replace with your source directory\n",
    "output_file = \"../data/combined_cavity-claw_data.json\"  # Replace with your desired output file path\n",
    "combine_json_files(source_directory, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the json files in data to \"LFL_USC_{hash}.json\" where the hash is based on the file contents\n",
    "\n",
    "for file in glob.glob(\"../data/*.json\"):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        hash = hashlib.md5(json.dumps(data, sort_keys=True).encode('utf-8')).hexdigest()\n",
    "        new_file = file.replace(\".json\", f\"LFL_USC_{hash}.json\")\n",
    "        os.rename(file, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /Users/shanto/.cache/huggingface/modules/datasets_modules/datasets/SQuADDS_DB/6c042e99be0aa47463aa5d9ae2ceeb87b02331a7e2aa159865c27f7b19e9316e\n",
      "Testing builder 'qubit-TransmonCross-cap_matrix' (1/3)\n",
      "Generating dataset s_qu_adds_db (/Users/shanto/.cache/huggingface/datasets/s_qu_adds_db/qubit-TransmonCross-cap_matrix/1.0.0/6c042e99be0aa47463aa5d9ae2ceeb87b02331a7e2aa159865c27f7b19e9316e)\n",
      "Downloading and preparing dataset s_qu_adds_db/qubit-TransmonCross-cap_matrix to /Users/shanto/.cache/huggingface/datasets/s_qu_adds_db/qubit-TransmonCross-cap_matrix/1.0.0/6c042e99be0aa47463aa5d9ae2ceeb87b02331a7e2aa159865c27f7b19e9316e...\n",
      "Downloading data files: 100%|██████████████████| 3/3 [00:00<00:00, 12458.33it/s]\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|█████████████████████| 3/3 [00:00<00:00, 853.43it/s]\n",
      "Generating train split\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shanto/miniconda3/envs/qiskit_metal/bin/datasets-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/shanto/miniconda3/envs/qiskit_metal/lib/python3.10/site-packages/datasets/commands/datasets_cli.py\", line 39, in main\n",
      "    service.run()\n",
      "  File \"/Users/shanto/miniconda3/envs/qiskit_metal/lib/python3.10/site-packages/datasets/commands/test.py\", line 146, in run\n",
      "    builder.download_and_prepare(\n",
      "  File \"/Users/shanto/miniconda3/envs/qiskit_metal/lib/python3.10/site-packages/datasets/builder.py\", line 948, in download_and_prepare\n",
      "    self._download_and_prepare(\n",
      "  File \"/Users/shanto/miniconda3/envs/qiskit_metal/lib/python3.10/site-packages/datasets/builder.py\", line 1043, in _download_and_prepare\n",
      "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
      "  File \"/Users/shanto/miniconda3/envs/qiskit_metal/lib/python3.10/site-packages/datasets/builder.py\", line 1445, in _prepare_split\n",
      "    raise NotImplementedError()\n",
      "NotImplementedError\n"
     ]
    }
   ],
   "source": [
    "!datasets-cli test ../data/SQuADDS_DB.py --save_info --all_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "def create_train_val_test_splits(source_files, output_directory, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Splits the data from source JSON files into train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    # Load data from all source files\n",
    "    for file_path in source_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            all_data.extend(data)\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(all_data)\n",
    "\n",
    "    # Split the data\n",
    "    total_data = len(all_data)\n",
    "    train_end = int(total_data * train_ratio)\n",
    "    val_end = train_end + int(total_data * val_ratio)\n",
    "\n",
    "    train_data = all_data[:train_end]\n",
    "    val_data = all_data[train_end:val_end]\n",
    "    test_data = all_data[val_end:]\n",
    "\n",
    "    # Save the splits\n",
    "    with open(os.path.join(output_directory, 'train.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_data, f, indent=4)\n",
    "\n",
    "    with open(os.path.join(output_directory, 'validation.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(val_data, f, indent=4)\n",
    "\n",
    "    with open(os.path.join(output_directory, 'test.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_data, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files = [\n",
    "    \"/Users/shanto/LFL/SQuADDS/SQuADDS/data/cavity_claw/LFL_USC_cavity_claw_3e95ba4a2e4da2141f9edaa9f9fa1653.json\",\n",
    "    \"/Users/shanto/LFL/SQuADDS/SQuADDS/data/coupler/LFL_USC_coupler_e7855e5c7467f76edb09779d8f3a1a0c.json\",\n",
    "    \"/Users/shanto/LFL/SQuADDS/SQuADDS/data/qubit/LFL_USC_qubit_e68f323df894ba4b2891bd64742a2c35.json\",\n",
    "]\n",
    "output_directory = '../data/'\n",
    "\n",
    "create_train_val_test_splits(source_files, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
